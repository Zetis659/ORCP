# ORCP — Optical Recognition of Cars and Prices
ORCP - это проект, цель которого - автоматизировать процесс идентификации автомобилей и определения их стоимости с использованием технологии оптического распознавания.
Этот проект включает в себя: сбор данных, обучение моделей и развёртывание Telegram бота для предоставления пользователям информации о марке и модели автомобиля, а так же его стоимости на основе фотографий автомобиля.

**Ссылки:**
- [Бот ORCP в Telegram](https://t.me/ORCPbot)
- [Веса модели классификации](https://drive.google.com/file/d/1E1mIXgHqpur8e2woJ86jKX7pHcCcFJic/view?usp=drive_link)
- [Веса модели детекции](https://drive.google.com/file/d/1S42IQZKm2Mnw6nsbGeST4Ao9LtwmlIYH/view?usp=drive_link)
- [Загруженные фото китайских авто для модели](https://drive.google.com/file/d/1220iqpGq6Xhwf4KEH9yDBuEo5QxL2IH9/view?usp=sharing)
- [Отобранные фото китайских авто для модели](https://drive.google.com/file/d/1uqsQV7Pkog9XilNDuZtm96ZEkBMcwA-1/view?usp=sharing)
- [Ссылки на фото китайских авто](https://drive.google.com/file/d/1-SriKOyJjJq_0TyK8bfAamKST8dGv6js/view?usp=sharing)
- [База загруженных авто](https://drive.google.com/file/d/1s6hamNrSZACgzBNVL7jES3M25XLlGWFT/view?usp=drive_link)


## Table of Contents
- [ORCP — Optical Recognition of Cars and Prices](#orcp--optical-recognition-of-cars-and-prices)
  - [Table of Contents](#table-of-contents)
  - [Project Overview](#project-overview)
  - [Usage](#usage)
    - [Parsing](#parsing)
      - [Необходимые библиотеки:](#необходимые-библиотеки)
      - [Необходимое стороннее ПО:](#необходимое-стороннее-по)
    - [Model Training - YOLOv8](#model-training---yolov8)
      - [Необходимые библиотеки:](#необходимые-библиотеки-1)
      - [Мой билд:](#мой-билд)
    - [Telegram bot](#telegram-bot)
      - [Необходимые библиотеки:](#необходимые-библиотеки-2)
  - [Data Collection](#data-collection)
      - [Логика парсинга:](#логика-парсинга)
    - [1.0 Preparing for parsing](#10-preparing-for-parsing)
    - [1.1 Parsing brands](#11-parsing-brands)
    - [1.2 Parsing models](#12-parsing-models)
    - [1.3 Parsing generations](#13-parsing-generations)
    - [1.4 Parsing ALL cars](#14-parsing-all-cars)
  - [Model Training](#model-training)
  - [Telegram Bot](#telegram-bot-1)
  - [License](#license)
## Project Overview
**ORCP** включает в себя три основных этапа:

1. **Сбор данных**: Написание парсера, который сохранят информацию об авто по различным параметрам: стоимость, объём двигателя, налог и т.д. Помимо этого парсер сохраняет ссылки на фотографии авто в текстовые файлы для дальнейшего обучения.
2. **Обучение модели**: Используя загруженные ссылки, скачать фотографии автомобилей, отобрать подходящие фотографии для обучения, разделить на обучающую и тестовую выборки и обучить на этих данных модель.
3. **Telegram bot**: Реализую Telegram бота, позволяющего взаимодействовать с пользователями. Пользователи могут отправлять фотографии боту, который используя обученные модели, будет предоставлять информацию о марке, модели, годах выпуска и стоимости авто.

## Usage
Использовать репозиторий **ORCP** можно по разному, по этому напишу отдельно про каждый этап. У каждого раздела, есть своя папка, которая находится в SRC, а именно Parsing, YOLOv8 и Bot. Основные детали при их использовании будут указаны ниже.
### Parsing
В папке Parsing, есть несколько модулей, с помощью которых можно загружать информацию об автомобилях с сайта "auto.ru". Здесь будут указаны необходимые библиотеки, которые нужны для запуска парсера, а так же дополнительное ПО. 

Для просмотра логики парсинга, откройте пункт [Data Collection](#data-collection)

**Парсер будет корректно работать, только при выполнении всех условий ↓ УКАЗАННЫХ НИЖЕ ↓**
#### Необходимые библиотеки:
> **fake_useragents** - замена агента браузера, при парсинге
> 
> **user_agents** - фильтрация мобильных агентов
> 
> **requests** - запросы на сайт
> 
> **tqdm** - подсчёт оставшегося времени, при парсинге
> 
> **random** - генерация случайных чисел, при смене IP
> 
> **genericpath** - проверка существования папки
> 
> **sys** - импорт модулей из разных папок
>
> **os** - указание пути для файлов
>
> **time** - подсчёт затраченного времени
>
> **csv** - сохранение *csv* файлов

#### Необходимое стороннее ПО:
Для постоянного изменения IP во избежании появления *капчи* использовался: ~~Tor~~ "луковый" браузер. **Без данного ПО, код работать НЕ будет!**

Для того, чтобы контролируемо менять IP адрес нужно:
1. Установить "луковый" браузер
2. В папке "лукового" браузера найти конфигурационный файл *torrc*
3. В данном файле прописать дополнительные порты. Я добавил 100 портов, начиная с "SocksPort 9050" и заканчивая "SocksPort 9149". Каждый порт должен идти с новой строчки.

**Парсер будет корректно работать, только при выполнении всех условий ↑ УКАЗАННЫХ ВЫШЕ ↑**
___
### Model Training - YOLOv8
В качестве основы, для классификации и детекции автомобилей, мне послужила модель нейросети **YOLOv8**.
#### Необходимые библиотеки:
> **torch == 2.0.1** - фреймворк для работы с нейросетями
>
> **nvidia-cuda == 11.7** - для использования видеокарт nvidia при обучении
>
> **ultralytics** - для импорта модели YOLO
>
> **requests** - для запросов на сайт
>
> **os** - указание пути файлов
>
> **shutil** - создание копий исходных файлов
>
> **time** - подсчёт затраченного времени
>
> **random** - генерация случайных числе, при выборе train/test
#### Мой билд:
- OS: **Manjaro Linux 23.0.3 Uranos, GNOME 44.5, Core 6.1.55-1-MANJARO**
- GPU: **Nvidia RTX 2070 8Gb**
- Nvidia driver: **470.199.02**, CUDA 11.4 (Pre-installed)
- CUDA: **11.8** (Installed after Nvidia driver)
- CUDNN: **8.6** (Installed after CUDA)
- Python: **3.10.12**
___
### Telegram bot
Для использования Telegram бота, основным инструментом была библиотека aoigram 3.0 и библиотека asyncio - для выполнения асинхронных задач.
#### Необходимые библиотеки:
> **torch == 2.0.1** - фреймворк для работы с нейросетями
>
> **nvidia-cuda == 11.7** - для использования видеокарт nvidia при обучении
>
> **ultralytics** - для импорта модели YOLO
>
> **aiogram == 3.00** - для использования Telegram API
>
> **asyncio** - для асинхронных функций бота (приём сообщений, фото и т.д.)
>
> **pandas** - для работы с *csv* файлами
>
> **sys** - импорт модулей из разных папок
>
> **os** - указание пути для файлов
>
> **time** - подсчёт затраченного времени
>
> **datetime** - сохранение фото в формате d-m-Y-H-M-S

## Data Collection
Для сбора данных мной был выбран сайт "auto.ru", так как это один из самых популярных сайтов для продажи авто. На момент парсинга (август 2023) на данной площадке было ≈370 000 объявлений о продаже авто, а на момент написания README (октябрь 2023) ≈410 000 объявлений.

Для использования парсера, нужно выполнить все пункты в разделе [Usage → Parsing](#parsing), а именно: установку "лукового" браузера, открытие портов и т.д.

Для удобства использования, все модули и данные имеющие отношение к парсингу расположены в: SRC/Parsing. А все модули python для парсинга находятся в папке: SRC/Parsing/Proxy, так как для парсинга используется proxy. Кроме того в данном разделе, модули специально названы в алфавитном порядке, для удобного последовательного запуска. Рекомендуемый порядок запуска: a1.., a2.., b1.., c1.. и т.д.
#### Логика парсинга:
**Бренд → Модель → Поколение → Страница → Объявление → Данные**

Более подробное описание будет ↓ ниже ↓
### 1.0 Preparing for parsing
После того, как все условия использования были выполнены, можно проверить корректно ли работают порты. Для этого вы можете запустить файл: Helper/tor_ip.py и если при каждом новом запуске ваш IP изменяется, то всё было настроено верно.

Если всё работает, то можно приступить к подготовке к парсингу. Начать следует с модуля *a1_preparation_to_parsing_PROXY.py*. В данном файле нужно:
1. Добавить модуль, в системный путь python (sys.path.append...) изменить на свой путь
2. Изменить переменную заголовка header, на актуальную. В противном случае *капча* будет постоянно прерывать парсинг. Для этого нужно:
   1. Открыть сайт "auto.ru", и перейти в раздел объявления
   2. Выбрать любую марку и модель авто, затем нажать на кнопку показать *n* предложений
   3. Открыть инструменты разработчика (обычно f12), затем выбрать пункт сеть
   4. Прокрутить страницу вниз и нажать на кнопку любой следующей страницы в поиске (например 2)
   5. После загрузки страницы в инструментах разработчика для удобства отфильтровать данные по убыванию размера и нажать на файл .json с названием "listing"
   6. Выбрать пункт заголовки и в нём найти подпункт: заголовки запроса
   7. Нажать на кнопку необработанные заголовки при наличии и скопировать все заголовки
   8. Вставить заголовки в переменную header 
   9. Удалить одну или несколько первых строк, так, чтобы первая строка была: "Host: auto.ru"

Запустите данный python модуль и если вы видите в терминале, вывод .json файла и далее надпись: "The header is set correctly!!!" - значит вы выполнили всё верно.

### 1.1 Parsing brands
После того, как подготовка была успешно завершена, можно приступить к парсингу. Для этого нужно запустить модуль: a2_parsing_brands_PROXY.py, который будет парсить 99 страниц сайта - это максимальное количество отображаемых страниц. На каждой странице расположены несколько объявлений, чаще всего их 37. При нахождении бренда, который ещё не был записан, он записывается в отдельный файл.

**Логика:**

**Первый цикл** бежит по 99 страницам, отсортированных по дате размещения, **второй цикл** бежит по объявлениям авто на каждой странице ~37 объявлений с авто, и **если  бренда ещё не было в списке, то добавить бренд в список и сохранить в список**. Файл называется brands.txt, находится в папке: SRC/Parsing/Results/Cars/Brands/brands.txt

**Рекомендую** сразу дублировать загруженные данные в папку:
SRC/Parsing/Results/Cars/BackUps, так как это будет удобно использовать в повторных парсингах при ошибках.

При необходимости можно изменить сортировку авто, для этого нужно изменить значение ключа "sort" в переменной params. Примеры сортировок: cr_date-desc - по дате размещения, autoru_exclusive-desc - по уникальности, fresh_relevance_1-desc - по актуальности
### 1.2 Parsing models
После того, как бренды были загружены в файл brands.txt, можно парсить сайт по моделям. Для этого нужно запустить модуль: *b1_parsing_models_PROXY.py*

**Логика:**

**Первый цикл** бежит по списку с брендами, который уже был загружен, **второй цикл** бежит по списку доступных страниц для данного бренда, **третий цикл** по объявлениям ~ 37 на странице. **Если модели ещё не было, то сохранить в файл с названием бренда.** Сортирую авто **по уникальности.**

Примеры: SRC/Parsing/Results/Cars/Models/GEELY.txt,
SRC/Parsing/Results/Cars/Models/HAVAL.txt

Во время парсинга моделей сохраняется файл, в котором будет указано, какие бренды и модели уже были загружены. Он расположен в: SRC/Parsing/Results/Cars/Parsing_failure/Models_fail.txt. При возникновении ошибки можно воспользоваться данным файлом,и удалить из файла brands.txt уже загруженные бренды, чтобы повторно не парсить одно и тоже. Не забывайте делать backup файлов перед его редактированием (дублирование в папку BackUps), потому что в дальнейшем, они понадобятся в исходном виде!

### 1.3 Parsing generations
После того, как были загружены бренды и модели, можно парсить сайт по поколениям данных автомобилей. Для этого нужно запустить модуль: *c1_parsing_generations_PROXY.py*

**Логика:**

**Первый цикл** бежит по списку с брендами, **второй цикл** по списку моделей, **третий** по страницам модели, **четвёртый** по объявлениям на каждой странице. **Если поколения ещё не было, то сохранить в файл с названием бренда и модели**. В папку **Generations** сохраняются **закодированные названия поколений**. Пример: SRC/Parsing/Results/Cars/Generations/HAVAL_F7.txt  **(21569049, 23206512)**. В папку **Generations_names** сохраняются **названия поколений**. Пример: SRC/Parsing/Results/Cars/Generations_names/HAVAL_F7.txt **(I,
I Рестайлинг)**

Во время парсинга поколений сохраняется файл, в котором будет указано, какие бренды модели и поколения уже были загружены. Он расположен в: SRC/Parsing/Results/Cars/Parsing_failure/Generations_fail.txt. При возникновении ошибки можно воспользоваться данным файлом,и удалить из файла brands.txt уже загруженные бренды, и модели из файла с моделями, чтобы повторно не парсить одно и тоже. Не забывайте делать backup файлов перед его редактированием (дублирование в папку BackUps), потому что в дальнейшем, они понадобятся в исходном виде!
### 1.4 Parsing ALL cars
После того, как были загружены бренды, модели и поколения можно приступить к парсингу всех автомобилей. Парсинг включает в себя 39 различных параметров, таких как: стоимость, объём двигателя, транспортный налог, тип кузова, разгон до сотни и многие другие. По мимо парсинга параметров авто, одновременно с ним происходит парсинг фотографий авто, а именно ссылок на фотографии. Ссылки на фото сохраняются в разных файлах с тремя разными разрешениями фотографий. Для парсинга нужно запустить модуль: *d1_parsing_all_cars_PROXY.py*

**Логика:**

**Первый цикл** бежит по списку с брендами, **второй** с моделями, **третий** с поколениями, **четвёртый** со страницами и **пятый** с объявлениями на каждой странице.
Каждый автомобиль соответствует одной строке в *csv* файле. Данные сохраняются в файл: SRC/Parsing/Results/Data/all_cars.csv


Ссылки на фото сохраняются в папке: SRC/Parsing/Results/Photo_links
Например ссылки на Haval F7 I поколения в разрешении 320x240 сохраняются в файл: SRC/Parsing/Results/Photo_links/HAVAL/F7/HAVAL_F7_I_2018-2022-320x240.txt



## Model Training
!!Описание обучения модели и анализа данных!!
## Telegram Bot
!!Описание использования Telegram юота!!
## License
License
ORCP is licensed under the MIT License - see the LICENSE file for details.
